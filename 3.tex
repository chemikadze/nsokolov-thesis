\chapter{РАЗРАБОТКА МОДУЛЕЙ}
% \chapter{Разработка модулей}

\section{Выбор стека технологий}

Перед началом реализации разработанной архитектуры необходимо выбрать
стек технологий, которые будут использоваться при разработке.
Необходимо определиться со следующими пунктами:
\begin{itemize}
    \item операционная система
    \item гипервизоры
    \item реализация управляющей логики
    \item API
\end{itemize}
Касательно гипервизоров, необходимо выбрать способ виртуализации обычных виртуальных машин
и сетевого оборудования.

В качестве операционной системы предпочтительнее всего использовать Linux, так как ее 
администрирование не представляет собой большой проблемы, она бесплатна и имеет
открытый исходный код и отличную поддержку большинства современных языков 
программирования. Кроме этого, для администрирования Linux существуют доказавшие
свою эффективность системы автоматизации установки и администрирования, что совершенно
необходимо для успешной поддержки многомашинных систем.
%TODO источники

Если говорить о виртуализации сетевого оборудования, то имеется два пути: использовать
специализированные гипервизоры, или же использовать стандартные гипервизоры вместе с
программными коммутаторами, такими как Open vSwitch. На текущий момент, программные
коммутаторы редко используются при создании сетей в предприятиях, поэтому большого
смысла поддерживать этот вариант нет. По данным Infonetics Research, Cisco занимает
лидирующие позиции на рынке сетевого оборудования,
% http://www.infonetics.com/pr/2012/3Q12-Service-Provider-Routers-Switches-Market-Highlights.asp
поэтому выбор этого сетевого оборудования в качестве цели для виртуализации вполне оправдан.
Кроме этого, гипервизор dynamips, предназначенный для эмуляции именно этого оборудования,
является самым распространенным и стабильным в своей области. %TODO источник

Среди гипервизоров PC-совместмого оборудования можно выделить несколько наиболее
популярных решений для Linux:
\begin{itemize}
    \item VmWare ESX
    \item VirtualBox
    \item Xen
    \item KVM
\end{itemize}

VmWare ESX является одним из наиболее популярных коммерческих гипервизоров, но при 
использовании в рамках данного проекта имеет ряд недостатков. Во-первых, гипервизор
работает только со специальным образом модифицированным ядром ОС, во-вторых,
данный гипервизор имеет закрытый код, и в случае необходимости становится невозможным
внести изменения, необходимые для реализации проекта.

VirtualBox -- гипервизор с открытым исходным кодом, в данный момент поддерживаемый 
компанией Oracle. Данный продукт в основном базируется как решение для настольных
компьютеров, несмотря на возможность работы без графического интерфейса.
Ранее VirtualBox разрабатывался компанией Sun, которая была куплена Oracle в 2010 году.
Переход многих программ с открытым исходным кодом во владение к Oracle сказался на
их поддержке крайне негативно, поэтому в долгосрочной перспективе использование
VirtualBox при реализации данного проекта не имеет большого смысла при наличии альтернатив.

Xen долгое время был выбором по-умолчанию в Linux-среде и активно поддерживался
самой большой Linux-компанией -- RedHat. %TODO источник
Тем не менее, последние несколько лет интерес к нему падает, так как
RedHat переключил свой фокус на KVM. Так же, в силу того, что этот гипервизор 
использует технику паравиртуализации, его администрирование имеет ряд отличий
от администрирования обычных Linux-систем.

KVM -- относительно новый, но уже достаточно популярный и стабильный Linux-специфичный
гипервизор. На данный момент он активно поддерживается, в том числе коммерческими компаниями,
такими как RedHat, и на настоящий момент является выбором по-умолчанию для виртуализации в 
Linux. Кроме этого, благодаря открытому коду, в него будет возможно внести изменения,
если такая необходимость возникнет во время разработки.
Таким образом, в качестве гипервизора PC-совместимых виртуальных машин в данном проекте
будет использоваться KVM.

При разработка управляющей логики имеется два варианта: писать весь необходимый код с нуля, 
используя только стандартные библиотеки языков, или же расширить возможности имеющейся
платформы. Написание кода с нуля с одной стороны позволяет получить полный контроль 
над кодовой базой и выбором технологий, но с другой стороны -- потребует реализации
большого объема относительно стандартной логики, которая не имеет большого отношения 
к сути данной работы. Гораздо более оптимальным вариантом является использование уже 
существующих наработок.

Как нельзя кстати в данном случае подходит платформа OpenStack. Рассматривая компоненты,
из которых она состоит, можно отметить, что архитектура этой платформы хорошо вписывается
в архитектуру, разработанную в предыдущей главе. Соответствие компонент OpenStack и 
предполагаемых компонент платформы моделирования сети можно увидеть на рис.~\ref{fig:openstack-lowlevel}.
\begin{figure}
  \centering
  {\footnotesize\input{fig/openstack-lowlevel}}
  \caption{Соответствие архитектуры OpenStack и архитектуры системы моделирования топологий}  
  \label{fig:openstack-lowlevel}
\end{figure}
Очевидно, что большинство компонент OpenStack может быть использовано в 
системе моделирования сетей либо напрямую, без всякой модификации, либо путем
написания соответствующего расширяющего кода. Список компонентов, подлежащих
повторному использованию, представлен в таблице~\ref{tab:openstack-reuse}
\begin{table}
\center
\caption{Возможность посторного использования компонент OpenStack}
\label{tab:openstack-reuse}
\begin{tabular}{|p{5cm}|p{4cm}|p{5cm}|} \hline 
\multicolumn{3}{|c|}{подсистема прикладного программного интерфейса} \\ \hline 
авторизация и аутентификация & quantum & без изменений\\ \hline
логика высокоуровневых команд & - & необходима реализация \\ \hline
\multicolumn{3}{|c|}{подсистема хранения данных} \\ \hline
хранение образов ОС & glance & без изменений \\ \hline
хранение данных виртуальных машин & на узлах nova-compute или в nova-volume & без изменений\\ \hline
\multicolumn{3}{|c|}{вычислительная подсистема} \\ \hline
подсистема планировки ресурсов & nova-scheduler & без изменений \\ \hline
подсистема сетевого взаимодействия & quantum & необходим дополнительный модуль \\ \hline
подсистема виртуализации & nova-compute & необходим дополнительный модуль \\ \hline
\hline 
\end{tabular} 
\end{table}

Путем использования уже существующей платформы, при реализации системы моделирования
сетей мы избегаем реализации большого количества не относящейся к сути логики. В итоге мы
получаем три основных модуля, требующих реализации:
\begin{enumerate}
    \item высокоуровневый прикладной программный интерфейс
    \item сетевое взаимодействие
    \item запуск специфических виртуальных машин -- маршрутизаторов в гипервизоре dynamips
\end{enumerate}

Прикладной программный интерфейс должен работать в терминах разработанной модели данных.
Следуя концепции модульной архитектуры, он будет реализован в виде отдельного процесса,
и будет транслировать высокоуровневые вызовы пользователя в ряд вызовов к нижележащим
подсистемам: nova-api и quantum.
%TODO технологии?

Модель сети, которая используется внутри OpenStack по-умолчанию устроена так, что 
все виртуальные машины одного проекта находятся в одной подсети. Тем не менее,
используя последние наработки сообщества разработчиков этой платформы, возможно использование
quantum для гибкой настройки соединения между виртуальными машинами.

Для того, чтобы конфигурировать сеть между виртуальными машинами в OpenStack на 
канальном уровне, необходимо реализовать два аспекта. Во-первых, необходим такой способ 
инкапсуляции кадров канального уровня, который позволит запускать достаточно большое
количество виртуальных машин в одной системе, обеспечивая изоляцию между ними.
Во вторых, необходимо предоставлять данные о физических портах в подсистему виртуализации.

Проекты GNS3 и dynagen решают проблему инкапсуляции при помощи использования UDP-каналов.
Для работы dynamips совместно с KVM при данном подходе ранее было необходимо использование 
специально модифицированной версии Qemu, но на настоящий момент UDP-каналы
поддерживаются и в официальной версии эмулятора.

Проблема предоставления данных о физических портах в систему виртуализации вытекает
из ограниченной сетевой модели OpenStack, которая использовалась с самого начала разработки
проекта. Вместо явного указания количества виртуальных сетевых адаптеров и способа их 
соединения с сетями, в nova-compute передавался только список сетей, к которым необходимо
подключить виртуальную машину. По этой причине не существует стандартного способа передачи
таких метаданных о подключенной сети, как номер сетевой карты и порта.
Для решения проблемы метаданных портов в рамках этого проекта создано расширение прикладного 
программного интерфейса Quantum, позволяющее задавать и получать метаданные порта.

Архитектура nova-compute изначально была расчитана на поддержку различных гипервизоров.
Благодаря этому, для поддержки виртуализации сетевого оборудования системой моделирования
сетей, необходимо создать так называемый драйвер гипервизора. Интерфейс драйвера гипервизора
содержит около 60 методов, из которых далеко не все обязательны для реализации, и
в нашем случае задача сводится к созданию простой обертки над dynagen, которая конфигурирует
виртуальные маршрутизаторы согласно информации, полученной от прикладного программного
интерфейса и расширения Quantum для метаданных.

\section{Описание прикладного программного интерфейса}
Интерфейс программирования приложений (иногда интерфейс прикладного программирования) (англ. application programming interface, API) --  набор готовых классов, процедур, функций, структур и констант, предоставляемых приложением (библиотекой, сервисом) для использования во внешних программных продуктах.

API определяет функциональность, которую предоставляет программа (модуль, библиотека), при этом абстрагируя конкретную реализацию этой функциональности. Благодаря API возможно создание
программ, взаимодействующих с другими программами.

Так как в рамках данной работы делается акцент на разработку логики системы моделирования сетей,
разработанная реализация не имеет графического пользовательского интерфейса. Вместо этого,
система предоставляет свой собственный API для взаимодействия, оперирующий в терминах 
модели предметной области.

На настоящий момент, наиболее часто используемым способом организации взаимодействия
программ по сети является REST. REST (Representational State Transfer, «передача представлений 
состояний») не имеет в основе конкретного стандарта, и является не более чем стилем построения 
архитектуры распределенного приложения. Принципы REST были впервые описаны в диссертации
одного из разработчиков протокола HTTP, Роя Филдинга.

Как правило, под REST сервисами подразумеваются сервисы, которые предоставляют пользователю
доступ к некоторому набору ресурсов, с которыми можно взаимодействовать посредством
HTTP-запросов. При этом ресурсы кодируются в одном или нескольких возможных заранее
определенных форматах, как правило человекочитаемых, таких как XML или JSON.
В данном случае в качестве формата используется JSON, так как он является более компактным
и легкочитаемым по сравнению с XML, а так же наиболее распространенным на данный момент.

Несмотря на то, что сам по себе REST не является стандартом, существуют стандарты на описание
интерфейсов REST-сервисов. Если для представления данных используется JSON, то можно
применить стандарты JSON Schema и JSON Hyper-Schema. Первый стандарт позволяет
регламентировать формат представления ресурсов: имена полей, типы данных и т.д. Второй
стандарт расширяет JSON Schema, позволяя задавать ссылки на связанные ресурсы, а так же
ссылки на действия над данным ресурсом.

OpenStack использует для аутентификации сервис Keystone. Перед работой с системой, пользователь
должен сначала получить так называемый токен у Keystone, который необходимо отправлять в 
заголовке HTTP-запроса \verb`HTTP_X_AUTH_TOKEN` при обращении к Nova, Glance и другим сервисам. 
В целях унификации, прикладной программный интерфейс системы моделирования так же будет 
использовать Keystone для своей работы.

Как и в OpenStack, пользователи системы сгруппированы в группы, называемые тенантами 
(англ. tenants). Так как с одной стороны, разные группы пользователей в перспективе могут иметь 
разные картотеки сущностей, а с другой стороны REST подразумевает, что содержимое 
ресурса полностью описывается его URL, то принято решение ко всем адресам сущностей добавить
в виде префикса идентификатор группы.

При выделении сущностей в ресурсы необходимо поддерживать баланс между гранулярностью
и денормализованностью. Слишком большая гранулярность делает описание API раздутым и сложным,
а слишком денормализованное -- увеличивает количество передаваемых данных, и зачастую 
требует большого количества кода для извлечения желаемых сущностей из представления ресурса.

Анализируя предполагаемую работу с системой, можно выделить следующие наиболее частые действия:
\begin{itemize}
   \item создание, получение и удаление топологии
   \item получение списка типов узлов для запуска новой топологии со стандартным 
            набором сетевых адаптеров для узла
   \item получение списка типов сетевых адаптеров для запуска новой топологии, в которой
            к узлам подключены сетевые адаптеры, отличные от конфигурации по-умолчанию
\end{itemize}

Таким образом, можно выделить три ресурса:
\begin{itemize}
    \item типы узлов
    \item типы сетевых адаптеров
    \item топологии
\end{itemize}
Первые два должны изменяться только административно, последние же -- самим пользователем.

\subsection{Типы сетевых адаптеров}

%TODO реализовать параметры в коде
Тип сетевого адаптера имеет три атрибута: имя $name$ и список портов $ports$. 
Доступные типы сетевых адаптеров представлены
в виде единого ресурса, благодаря чему при редактировании топологии можно единожды
запросить список адаптеров и использовать его в дальнейшем.  Для упрощения кода на стороне 
пользователя, множество доступных адаптеров задается в виде объекта JSON с ключами,
равными имени типа адаптера. URL для ресурса будет иметь вид "/{tenantid}/cards", ресурс не изменяем
для пользователей. 

JSON-схема ресурса, таким образом, принимает следующий вид:

\lstinputlisting{schema/schema_slots.json}

Что соответствует подобному JSON-документу:

\lstinputlisting{code/slots.json}


\subsection{Типы узлов}

%TODO реализовать метаданные, параметры и os
%TODO исчез id из слотов
Типы узлов имеют следующие аттрибуты: идентификатор $id$, человекочитаемое название
$name$, список портов для подключения сетевых адаптеров $slots$, 
набор параметров $parameters$, список доступных для запуска версий операционных систем 
$software$, а так же набор метаданных $metadata$. Сетевые адаптеры
по-умолчанию указаны напрямую в списке портов.
Так же, как и типы сетевых адаптеров, типы представлены в виде единого ресурса, объединенные в 
объект JSON. URL ресурса -- "/{tenantid}/devices", для пользователя этот ресурс доступен только для чтения.

JSON-схема типов узлов имеет следующий вид:

\lstinputlisting{schema/schema_hardware.json}

Что соответствует следующему JSON-документу:

\lstinputlisting{code/hardware.json}


\subsection{Топологии}

%TODO добавить софт
%TODO добавить параметры
%TODO добавить URL'ы консолей

Топологию, как было рассмотрено ранее, можно разделить на список узлов и список 
связей. При этом каждый узел имеет идентификатор $id$, человекочитаемое название
$name$, список слотов с подключенными адаптерами $slots$, 
набор значений параметров $parameters$, версию операционной системы $software$, 
а так же набор метаданных $metadata$. 

В целях упрощения реализации будем считать, что каждая связь может иметь только два 
подключенных порта  $left$ и $right$, и не имеет каких-либо специфических параметров.
Так же, каждое соединение имеет идентификатор связи $id$.

Топологии являются единственными ресурсами, которые может редактировать пользователь.
Они доступны по URL "/{tenantid}/instances/{id}", и могут быть созданы, прочтены с сервера и удалены.
Так же, у топологии существует дочерние ресурсы -- веб-консоли для устройств,
содержащие единственное поле адреса, и доступные по URL вида "/{tenantid}/instances/{id}/consoles/{device}".
В виде JSON-схемы вышеописанные требования принимают следующий вид:

\lstinputlisting{schema/schema_instances.json}

Что соответствует следующему JSON-документу:

\lstinputlisting{code/instances.json}

%TODO коды возврата HTTP

\section{Подсистема прикладного программного интерфейса}
Подсистема прикладного программного интерфейса системы моделирования достаточно проста,
основная ее задача -- по описанию топологии из запроса отправить некоторое количество
запросов сервисам OpenStack для настройки сети и вычислительных узлов.
При этом на стороне прикладного программного интерфейса нет никакой необходимости 
иметь какую-либо логику для обращения к Keystone в целях проверки аутентификации пользователя,
так как эту проверку будут выполнять сервисы OpenStack, получающие такие же заголовки
аутентификации, что и сам прикладной программный интерфейс.

В случае ошибки при выполнении любого запроса, результат успешно завершившихся запросов
будет откачен, а пользователь получит соответствующий код ошибки и краткое текстовое объяснение.

В подсистеме прикладного программного интерфейса можно выделить следующие компоненты:
\begin{itemize}
    \item контроллер действий
    \item отображение внутреннего представления во внешнее
    \item логика управления топологиями
    \item хранение моделей топологий
\end{itemize}

Контроллер содержит в себе логику обработки запроса и реализован при помощи микрофреймворка
Flask. При использовании данной технологии, обработчики запросов являются функциями в модуле
со специальными аннотациями. К примеру, получение списка запущенных топологий реализовано
следующим образом:
\begin{lstlisting}
@app.route("/<tenant>/instances", methods=["GET"])
def instance_list(tenant):
    instances = 
        app.facade.get_instances(RequestContext(request))
    return api_response(
        status=200, 
        payload=map(instance_to_json, instances))
\end{lstlisting}
Из полученного запроса извлекается OpenStack-специфичный контекст, который передается
в фасад, реализующий логику управления топологиями (в данном случае -- получение списка 
топологий). При этом модели топологий отображаются во внешнее представление
при помощи функции \verb`instance_to_json`. Логика отображения представлений достаточно
проста и не требует пояснений, так как сводится к формированию одних JSON-документов из других.

Логика управления топологиями предоставляет следующие операции:
\begin{itemize}
    \item \verb`launch_instance` (запуск топологии)
    \item \verb`destroy_instance` (остановка топологии)
    \item \verb`update_instance_status` (обновление статуса отдельных узлов)
    \item \verb`get_instance` (получение хранимого представления топологии)
    \item \verb`get_instances` (получение всех хранимых топологий)
    \item \verb`get_network_cards` (получение всех типов сетевых адаптеров)
    \item \verb`get_device_types` (получение всех типов узлов)
\end{itemize}

Операции над топологиями выполняют несколько запросов к сервисам OpenStack, по одному для 
каждого узла и каждой связи.
В качестве примера рассмотрим псевдокод запуска топологии:
\begin{lstlisting}
def launch_instance(self, ctx, instance):
  conn_info = DeviceConnections()
  created_networks = []
  created_devices = []
  try:
    for wire in instance['wires']:
      net = create_network(wire)
      created_networks.append(net)
      lport = create_port(instance, net, wire)
      rport = create_port(instance, net, wire)
      conn_info.add(wire['left'], lport)
      conn_info.add(wire['right'], rport)
    for device in instance['devices']:
      conn = conn_info.get(device['id'])
      device = create_device(device, conn)
      created_devices.append(device)
  except Exception as e:
    delete_devices_ignoring_errors(created_devices)
    delete_networks_ignoring_errors(created_devices)
    raise
  db.create_instance(instance)
  return instance
\end{lstlisting}
Объект \verb`conn_info` используется только для хранения промежуточной информации о 
подключениях.
Вся логика сводится к созданию соответствующих соединениям сетей в Quantum, и запуску
узлов в OpenStack, подключенных к соответствующим сетям. В случае любых ошибок
созданные устройства и сети удаляются.

Операция \verb`get_network_cards` не требуют запросов к OpenStack, и использует для выполнения 
список сетевых карт, предоставляемый библиотекой dynamips. 

%TODO фильтрация типов по флейворам
%TODO фильтация операционных систем
Операция получения всех типов узлов \verb`get_device_types`, в свою очередь,
так же использует dynamips для получения списка виртуального сетевого оборудования.
При этом список поддерживаемого оборудования фильтруется по списку типов виртуальных
машин OpenStack -- так называемых флейворов. Таким образом из поддерживаемого
dynamips оборудования в выдачу попадают только те, которые имеют соответствующую запись
в списке флейворов OpenStack. Формат флейворов для виртуального сетевого оборудования 
имеет вид \verb`r1.{model}`, где \verb`model` -- название модели устройства, к примеру 
\verb`c2610`.

Кроме этого, операция получения списка типов узлов совершает запрос к glance для того, чтобы 
найти список поддерживаемых операционных систем. Для каждого сетевого устройств будут указаны
только те операционные системы, чье название содержит идентификатор платформы
типа устройства, к примеру \verb`c2600` для модели \verb`c2610`.

Те флейворы, которые начинаются с префикса, отличного от \verb`r1`, к примеру
\verb`m1.small`, считаются обычными PC-совместимыми виртуальными машинами. 
При этом, в качестве образов операционных систем отображаются все хранимые образы, 
чье имя не совпало с названием ни с одной платформы.

\section{Подсистема сетевого взаимодействия}
TBD

\section{Подсистема виртуализации}
TBD

\section{Физическая топология}
TBD


