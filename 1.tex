\chapter{ОБЗОР ТЕХНОЛОГИЙ}
%\chapter{Обзор технологий}

\section{Виртуализация IBM-PC совместимого оборудования}

\subsection{XEN}

% этой секции нужна ревизия -- очень путанно и непонятно

В традиционных гипервизорах виртуальное аппаратное обеспечение представлено в 
виде, функционально идентичном реальному оборудованию, что позволяет запускать
внутри виртуальных машин немодифицированные версии операционных систем. Тем не менее,
у этого способа есть и свои минусы, особенно при эмуляции архитектуры x86. К примеру,
определенные инструкции машинных кодов должны перехватываться гипервизором
и выполняться особым образом. Так же, эффективная реализация блока управления памятью
(MMU) так же представляет отдельную сложность. Несмотря на трудности, эти и другие 
проблемы могут быть решены, но ценой этому будут сложность реализации и производительность.

Кроме специфичных для x86 архитектур проблем, против полной виртуализации существуют
и другие контраргументы. В частности, зачастую желательно, чтобы гостевая операционная
система видела не только виртуальные, но и реальные ресурсы. К примеру, при решении
задач, требовательных к точности временных отсчетов, может быть полезно предоставлять
в гостевой операционной системе реальное время наряду с внутренним виртуальным.

Чтобы избежать всех этих проблем, в XEN используется абстракция над реальным аппаратным
обеспечением, называемая паравиртуализацией. Позволяя достичь большей производительности,
эта техника тем не менее требует некоторых модификаций в гостевой операционной системе.
При этом стоит обратить внимание, что изменения в операционной системе не затрагивают
прикладного двоичного интерфейса (application binary interface, ABI), так что в гостевые
приложения в модификации не нуждаются.

% TODO: что такое домен

Вместо эмуляции реального аппаратного обеспечения, XEN предоставляет для операционной
системы набор простых абстракций над устройствами, что позволяет достичь большей
эффективности. В частности, весь ввод-вывод между доменами происходит через разделяемую
память при помощи асинхронных колец дескрипторов буфера. С одной стороны это позволяет
достичь больших скоростей при передаче данных, а с другой -- проводить различные 
проверки безопасности на стороне гипервизора, к примеру, что дескриптор буфера указывает
на реальную область памяти домена, от которого поступил запрос на запись.

Через всю архитектуру XEN проходит принцип отделения политик от механизмов. Несмотря на
то, что гипервизор обязан участвовать во всех низкоуровневых операциях вроде планировки
процессорного времени между виртуальными машинами, фильтровке сетевых пакетов перед
отправкой или проверок прав доступа при чтении данных, совершенно не требуется, чтобы 
в нем была реализована логика распределения процессорного времени или правила фильтрации
пакетов.

В результате, сам XEN предоставляет только набор самых базовых управляющих операций.
Доступ к этим операциям имеется только из специальных авторизованных доменов. Все 
вышеназванные правила при этом выполняются специальной управляющей программой, 
запущенной в таком управляющем домене, а не превилегированным кодом гипервизора.

Стоит обратить внимание, что домен, работающий с управляющим интерфейсом запускается
в момент загрузки XEN-хоста. Этот домен, называемый Domain0, как раз и содержит в себе
управляющие программы. Так же, программы запущенные в нем предоставляют набор операций
по манипуляции другими доменами, доступ к конифгурационным параметрам и политикам.

Кроме управления ресурсами процессора и памяти, управляющий интерфейс поддерживает
создание и удаление виртуальных сетевых интерфейсов (virtual network interfaces -- VIFs) и 
блочных устройств (virtual block devices -- VBDs). С этими виртуальные устройствами ввода-вывода
связывается набор политик, в которых указан набор доменов, которые имеют к ним доступ,
а так же какие операции над ними позволено производить.
\cite{Barham:2003:XAV:1165389.945462}

\subsection{QEMU}

QEMU -- пример классической полной виртуализации; основывается на базе динамического
транслятора. Он эмулирует несколько архитектур процессора: x86, PowerPC, ARM и SPARC на
большом количестве физического аппаратного обеспечения: x86, PowerPC, ARM, SPARC, Alpha
и MIPS, причем целевая архитектура и физическая архитектура могут не совпадать. К сожалению,
в силу вышеперечисленных возможностей, выполнение кода внутри данного гипервизора
замедляется в среднем от 4 до 10 раз.\cite{Bellard:2005:QFP:1247360.1247401}

Логически в QEMU можно выделить несколько основных подсистем:
\begin{itemize}
    \item Эмулятор процессора
    \item Эмулированные устройства (VGA-экран, последовательный порт, клавиатура и мышь PS/2,
     жесткий диск IDE, сетевая карта NE2000)
    \item Обычные устройства (блочные, символьные, сетевые) для соединения эмулятора с 
    реальными устройствами на хостовой машине.
\end{itemize}
Стоит отметить, что благодаря вышеперечисленному делению на подсистемы и открытым кодам,
QEMU послужил основой для ряда производных работ.
\cite{Raghav:2012:FSS:2159430.2159442} 
\cite{Becker:2012:XEQ:2380356.2380368} 
\cite{Hong:2012:HMR:2259016.2259030}
\cite{Ding:2011:PPS:2117686.2118470}
\cite{Nakamoto:2009:PUE:1524877.1525243}
% TODO: перечислить

Так как QEMU представляет реализует полную виртуализацию, на нем могут быть запущены
не модифицированные операционные системы со стандартным набором драйверов.
Сам QEMU работает на трех наиболее популярных операционных системах: Windows, 
Linux и Mac OS X.

Этот гипервизор может применяться для запуска приложений, предназначенных для 
операционной системы, отличной от ОС хост-машины, для тестирования поведения
программ, а так же в качестве "песочницы" при работе с недоверенным кодом.

Кроме этого, интересной особенностью этого гипервизора является поддержка в 
режиме эмулятора процессора. Этот режим является своеобразным подмножеством полного
эмулятора, позволяя запускать процессы из бинарных файлов архитектур, отличных от 
архитектуры хост-машины в контексте хостовой операционной системы. Одним из применений 
этого режима является проверка результата работы кросс-компиляторов без необходимости
запускать виртуальную машину с полной копией операционной системы.

\subsection{KVM}

KVM (Kernel Virtual Machine) -- относительно новый среди представленных гипервизоров,
относительно простой, но тем не менее обладающий очень высокими характеристиками. 
KVM, так же как и QEMU,
является реализацией полной виртуализации, но при этом полностью использует возможности 
аппаратного ускорения виртуализации современных процессоров.
\cite{Habib:2008:VK:1344209.1344217}

Разработчики KVM, вместо реализации больших частей операционных систем в своем
гипервизоре, как сделали разработчики других гипервизоров, нашли способ как заставить
работать ядро Linux в качестве гипервизора, что было достигнуто путем реализации
KVM в виде модуля ядра. Интеграция возможностей по виртуализации в ядро позволило
упростить работу с ресурсами в гипервизоре, а так же улучшить производительность в целом.
 
Данный подход имеет множество преимуществ. Используя ядро Linux в качестве основы для
своей работы, разработчики смогли избавить себя от создания таких подсистем, как
планировщик задач, так как в данной реализации виртуальная машина представлена в системе
обычным процессом. Достичь этого удалось с помощью объявления нового режима исполнения 
кода. Традиционно Linux использует два режима: ядра и пользователя, KVM же добавляет 
промежуточный гостевой режим.
 
Типичная установка KVM состоит из следующих компонент:

\begin{itemize}
    \item Драйвер для управления виртуальными устройствами, управление которым доступно
    через символьное устройство /dev/kvm.
    \item Программа для управления виртуальным аппаратным обеспечением, как правило
    специальной версией QEMU.
    \item Модулем ввода вывода, как правило эти возможности так же полностью предоставлены
    QEMU.
\end{itemize}

Интересной возможностью KVM, которая реализована благодаря полному доступу гипервизора
к внутренним механизмам ядра, является объединение одинаковых страниц 
(Kernel Same-page Merging -- KSM). KSM сканирует память виртуальных машин на предмет
одинаковых страниц памяти, и объединяет их в одну, так что одна и та же область памяти
используется несколькими виртуальными машинами. Если гость пытается изменить такую 
разделяемую область памяти, то для него будет вновь создана уникальная копия.

% TODO: здесь можно написать про то, как я делал ето и сколько оно дает

Используя эту функцию KVM в случаях, когда запускается много виртуальных
машин с одинаковой версией операционной системы, можно достичь большой экономии 
памяти, так как в данном случае создается
большое количество страниц с одинаковым содержимым, в котором находится в основном
неизменяемый бинарный код.

Еще одним преимуществом реализации KVM в виде модуля ядра Linux является превосходная
поддержка большого количества аппаратного обеспечения, так как все устройства, 
поддерживаемые данной операционной системой, могут быть использованы совместно
с гипервизором.
\cite{RedHat:kvm}



\section{Моделирование сети}

\subsection{ns2}

При исследованиях в области компьютерных сетей симуляции играют большую роль, так как
имеют много преимуществ по сравнению с проверками на реальном железе.
Сетевые симуляторы позволяют реализовать и изучать различные сетевые сущности в 
смоделированном окружении, что особенно хорошо при исследовании новых протоколов,
технологий, физических моделей и топологий.

% TODO: что такое дискретный?
NS-2 -- это дискретный сетевой симулятор, который продолжительное время был стандартом
де-факто при академических исследованиях.

В NS-2 широко используется язык Tcl, так как на момент создания этого симулятора
компиляция C++ файлов требовала относительно много времени. C++ использовался для
основных элементов и моделей, которые относительно стабильные и не требуют частых
изменений. Интерпретируемый язык Tcl же использовался для описания сценариев 
экспериментов, позволяя избежать долгой фазы компиляции.
\cite{Font:2010:ADS:1878537.1878651}

На настоящий момент имеющееся в доступе аппаратное обеспечение обеспечивает
адекватное время компиляции, поэтому преимущества от использования языка Tcl 
практически сошли на нет.

NS-2 разделяет структуру предметной области на сетевую топологию и агентов, которые
обмениваются данными через нее. Топология образована узлами и ребрами между ними,
причем с ребрами могут быть связаны объекты, которые описывают поведение соединения
между узлами, а так же характеристики канала и другие параметры.
Агенты являются по сути сетевыми приложения, которые запущены на узлах. Разработчик
модели сам выбирает формат данных, и при необходимости может зарегистрировать
в NS-2 свой собственный формат.

Архитектуру NS-2 можно разделить на 5 частей:
\begin{enumerate}
    \item Планировщик событий
    \item Сетевые компоненты, такие как узел и ребро
    \item Tcl
    \item OTcl -- объектное расширение к Tcl
    \item TclCL -- обертку вокруг OTcl для работы с объектами из C++
\end{enumerate}

Моделирование NS-2 можно классифицировать как дискретно-событийное.
Дискретно-событийное моделирование -- это вид имитационного моделирования, при 
котором функционирование системы представляется как хронологическая последовательность 
событий. Событие происходит в определенный момент времени и знаменует собой изменение 
состояния системы.

Планировщик предназначен для управления выполнением симуляции, и отвечает за выполнение
событий в определенные моменты времени.
Планировщики в NS-2 подразделяются на планировщики реального времени и
планировщики виртуального времени (non-real-time schedulers). Планировщики реального
времени используются при эмуляции сети, когда симулятор взаимодействует с реальной
внешней сетью.

Известной проблемой NS-2, к которая послужила причиной его устаревания является
необходимость редактирования C++ кода самого симулятора при разработке новых форматов
данных и протоколов. Так как эта проблема была заложена в самой архитектуре симулятора, 
единственным методом ее решения явилось полное переписывание.

\subsection{ns3}

NS-3 является дискретно-событийным симулятором для сетевых исследований,
и продолжает родословную NS-2. В отличии от предшественника, в котором все симуляции
описывались на языке Tcl, в данном случае их можно описывать как на C++, так и на Python.
Так как NS-3 был полностью написан с нуля, старые симуляции NS-2 не могут быть на нем
запущены.


При создании NS-3 разработчики выбрали целью улучшить следующие показатели NS-2:
\begin{itemize}
    \item модульность компонент
    \item масштабируемость симуляций
    \item интеграция со сторонним кодом
    \item поддержка эмуляции
    \item трассировка и статистика
    \item валидация
\end{itemize}

Так как большое количество исследований требует расширения возможностей
симулятора, NS-3 поддерживает расширение базовой функциональности путем
наследования базовых классов из стандартной реализации, при этом не требуя 
полной перекомпиляции самого симулятора.

Так же, благодаря правильному проектированию, NS-3 позволяет повторно использовать
части моделей, написанные для других симуляций. В основном это достигнуто за счет
того, что большая часть общего кода написана в терминах указателей на общие базовые классы,
из-за чего возможно совместное использование стандартных и сторонних модулей без 
изменения основного кода.

Другой новой функций NS-3 является трассировка, написанная с использованием
функций обратного вызова, что позволяет отделить код трассировки от общего кода.
В качестве формата данных используется libpcap, так как существует много уже существующих
утилит для работы с ним.
% TODO: источник?

Еще одним новшеством NS-3 является лучшая масштабируемость симуляций. Среди используемых
техник можно перечислить распределенные симуляций на базе протоколов PDNS и GTNetS,
кэширование результатов большой вычислительной емкости и более гибкую инфраструктуру
трассировки.

\subsection{dynamips -- эмулятор виртуального сетевого оборудования}

Несмотря на несомненную полезность сетевых симуляторов в академических исследованиях
сетевых протоколов, они, тем не менее, не позволяют моделировать реально существующие
модели оборудования и особенности реализаций стеков протоколов различных операционных
систем. Кроме этого, даже будучи заложенной в симулирующую программу, эта логика
может иметь отличия от реального прообраза.
Решить эти проблемы может только виртуализация оборудования.

% TODO: источник
Одним из наиболее популярных вендоров сетевого оборудования является компания Cisco,
известная так же своими сертификационными экзаменами. Зачастую у готовящихся
к экзамену нет доступа к реальному оборудованию, и долгое время единственными решениями
этой проблемы было либо прохождение дорогостоящих курсов в учебных центрах или 
использование специального ПО для симуляций, что так же не всегда удобно по 
вышеперечисленным причинам.

Эмулятор Dynamips\cite{website:dynamips} используется для виртуализации маршрутизаторов и 
коммутаторов фирмы Cisco. 
Dynamips позволяет загружать реальные образы Cisco Internetworking Operating 
System (Cisco IOS) и проводить
настройку виртуального аппаратного обеспечения таким же образом, как и реального.
\cite{Zhang:2011:CCV:1975507.1976866}

Dynamips может работать в двух режимах -- обычном и режиме гипервизора. В обычном режиме
каждая виртуальная машина запускается в своем собственном процессе, и не может быть
сконфигурирована в ходе работы. Режим же гипервизора позволяет запускать несколько 
виртуальных машин в одном процессе и производить конфигурацию виртуальной машины 
в ходе работы виртуального устройства.

Для связи портов виртуальной машины с внешним миром Dynamips предоставляет абстракцию
NIO (Network Input-Output), позволяя использовать различные способы для установления
соединения между виртуальными машинами или даже реальными интерфейсами реального
физического оборудования. На данный момент поддерживаются следующие виды:
\begin{itemize}
    \item UNIX сокеты
    \item User Mode Linux и Virtual Distributed Ethernet  % TODO ссылка
    \item Виртуальные сетевые интерфейсы linux (tap-устройства)
    \item Реальные сетевые интерфейсы
    \item Инкапсуляция в UDP-каналы
    \item Инкапсуляция в TCP-каналы
\end{itemize}

К сожалению, динамическая трансляция, которую выполняет dynamips, накладывает свои
ограничения на производительность виртуальных сетевых устройств. Кроме того,
некоторые модели оборудования Cisco не могут быть в нем сэмулированы, так как представляют
из себя композицию нескольких параллельно работающих устройств.

Тем не менее, для того ряда устройств, которые официально поддерживаются, обеспечена
полная работоспособность всех функций, доступных на реальном оборудовании. К ним
относятся:
\begin{itemize}
    \item 7000
    \item 3600 (3620, 3640 и 3660)
    \item 3700 (3725 и 3745)
    \item 2600 (от 2610 до 2650XM, 2691)
\end{itemize}

Типичный процесс работы выглядит следующим образом.
\begin{enumerate}
    \item Пользователь запускает несколько виртуальных машин dynamips, указывая тип
    эмулируемого оборудования и способ их соединения. К примеру, может использоваться
    UDP NIO, для чего необходимо выбрать номера портов для каждого соединяемого
    виртуального порта каждого устройства, и индивидуально для каждого порта указать
    локальный и удаленный адрес соединения.
    Так же необходимо указать номер порта, на котором будет доступна виртуальная консоль.
    \item Dynamips загружает образ операционной системы IOS, запускает его, и инициализирует
    NIO на указанных портах, в данном случае создавая UDP-сокеты с переданными параметрами.
    \item Пользователь подключается с помощью программы удаленного терминала (например,
    telnet или putty) и производит работу с консолью IOS.
\end{enumerate}

Так как процесс настройки NIO достаточно трудоемок и требует внимательности, для облегчения
этого была написана программа Dynagen.\cite{website:dynagen} Она состоит из двух частей:
низкоуровневой реализации API гипервизора dynamips на языке python и высокоуровневого
модуля, который использует файлы конфигурации с относительно простым синтаксисом
для задания взаимосвязей между устройствами.

Тем не менее не для всех может быть удобен процесс работы с файлами конфигурации, 
поэтому существует еще один, более высокоуровневый проект -- GNS 3.\cite{website:gns3}
Он представляет собой простой графический интерфейс, написанный на python и Qt, и использует
для запуска топологий Dynagen.

\section{Облачные системы}

%\subsection{Eucalyptus}
%Eucalyptus первоначально был создан в качестве платформы для так называемых "частных
%облаков". С самого начала он разрабатывался полностью совместимым с Amazon EC2 API,
%чтобы пользователи имели возможность мигрировать от Amazon в свое приватное 
%облако и наоборот без переписывания кода.
%
%Eucalyptus состоит из пяти компонентов:
%\begin{enumerate}
%    \item Контроллер облака (Cloud Controller -- CLC), который управляет виртуализованными ресурсами
%    \item Контроллер кластера (Cluster Controller -- CC), который управляет запуском виртуальных машин
%    \item Система хранения данных Walrus 
%    \item Контроллер хранения данных (Storage Controller -- SC), предоставляющий блочное     
%    хранилище
%    \item Контроллер узлов (Node Controller -- NC), специальная служба устанавливаемая на
%    все узлы, предназначенные для запуска виртуальных машин
%\end{enumerate}
%Эти компоненты распределены по вычислительным узлам системы, и конкретная топология
%зависит от конкретного случая.
%
%Eucalyptus имеет несколько режимов работы сети: управляемая (managed), 
%управляемая без LAN (managed noLAN), системная и статическая. В первых двух случаях
%пользователь управляет сетью вручную, с той разницей что в первом случае сети изолированы
%с помощью vLAN. В системном режиме виртуальные машины получают адреса от внешнего 
%DHCP-сервера. В статическом режиме адреса выдаются DHCP-сервером, управляемым из 
%Eucalyptus.
%
%\cite{vonLaszewski:2012:CMC:2353730.2353779}
%
%\subsection{OpenNebula}
%
%OpenNebula -- еще один проект по созданию IaaS-платформы с открытым исходным кодом.
%Данный проект отличается гибкостью и простотой, и при этом хорошей 
%функциональностью.
%
%Кроме этого, OpenNebula может работать в распределенном окружении, включающем
%в себя несколько установок системы в различных местах, благодаря чему можно легко
%создавать системы, устойчивые к отказам в каком-либо конкретном датацентре. При этом,
%работая в таком режиме, все подсистемы-участники управляются единообразно из одной точки
%доступа.\cite{vonLaszewski:2012:CMC:2353730.2353779}
%
%% поддержка API и распределения
%
%Простота данной системы достигнута ценой введения единой точки отказа. Все управление
%вычислительными ресурсами ведется с помощью одной программы, запущенной на 
%управляющем узле. При этом все управление ресурсами ведется с помощью протокола ssh.
%
%Работа с дисками виртуальных машин так же отличается простотой и гибкостью. Поддерживаются
%следующие режимы:
%\begin{itemize}
%    \item Диски хранятся как обычные файлы на локальных дисках, образа для запуска копируются вручную на вычислительные узлы.
%    \item Диски хранятся как файлы в разделяемой распределенной ФС (к примеру, NFS).
%    \item Диски являются разделами LVM (Linux Volume Manager).
%\end{itemize}
%
%Имеется так же три режима работы с сетью:
%\begin{itemize}
%   \item изоляция с помощью vLAN
%   \item изоляция с помощью ebtables (фаервол виртуальных сетевых мостов Linux)
%   \item использование программного коммутатора Open vSwitch
%\end{itemize}

\subsection{OpenStack}
OpenStack -- относительно молодой проект с открытым исходным кодом, 
объединяющий платформу для запуска виртуальных машин Nova, хранилище 
данных Swift, хранилище образов дисков Glance и несколько других проектов.
\cite{vonLaszewski:2012:CMC:2353730.2353779}

Начало проекта было дано хостинг-провайдером Rackspace и NASA, когда в июле 2010 года
были открыты исходные коды Rackspace Cloud Files -- родоначальника проекта Swift, 
и внутренних разработок NASA в сфере IaaS -- родоначальника проекта Nova.
Со временем к проекту подключилось множество других компаний, в числе которых
Canonical, Cisco, HP и Intel.

OpenStack Nova подразделяется на следующие компоненты:
\begin{itemize}
    \item nova-api -- REST-интерфейс для управления виртуальными машинами
    \item nova-scheduler -- планировщик ресурсов
    \item nova-compute -- сервис, управляющий виртуальными машинами на узлах
    \item nova-network -- менеджер сетей
    \item nova-volume -- предоставляет блочное хранилище для виртуальных машин
    \item nova-consoleauth, nova-novncproxy и другие -- удаленный доступ по протоколу VNC
\end{itemize}
Эти компоненты связаны с помощью сервиса обмена сообщений RabbitMQ, и используют
для хранения данных SQL-совместимые СУБД при помощи ORM-библиотеки SQLAlchemy.

Данный проект является хорошим примером модульного дизайна. 
Каждый сервис состоит из двух частей: обертка для поддержки распределенных вызовов,
которая является единой для всех сервисов, и класса-менеджера, реализующего логику работы.
При этом для каждого сервиса реализацию менеджера легко подменить, указав необходимое
имя класса в конфигурационном файле.

Следующим уровнем абстракции являются так называемые драйвера. Некоторые менеджеры,
такие как nova-compute и nova-scheduler, с одной стороны должны поддерживать достаточно 
большое количество логики, которая не представляют интереса при конфигурации.
С другой стороны часть логики может иметь достаточно много альтернативных реализаций,
к примеру различные алгоритмы распределения виртуальных машин по узлам или
поддержка новых гипервизоров. В таких случаях общая логика реализуется в менеджере,
а специфичные функции выделяются в драйвер. Конкретная реализация драйвера, как и в случае
с менеджером, указывается в конфигурационном файле.

Работа с OpenStack выглядит следующим образом:
\begin{itemize}
    \item Пользователь при помощи утилиты glance загружает специально подготовленный 
    образ диска для виртуальной машины, получая при этом идентификатор диска.
    \item При помощи утилиты nova, пользователь запускает нужную ему виртуальную машину,
    указывая идентификатор диска, так называемый flavor -- набор параметров, описывающий
    характеристики виртуальной машины, такие как количество виртуальных процессоров, 
    количество ОЗУ и ПЗУ, публичную часть SSH-ключа для работы с этой машиной и некоторые
    другие параметры.
    \item Запрос на запуск обрабатывается nova-api, и отправляется в nova-scheduler.
    \item nova-scheduler определяет узел, который может быть использован для запуска 
    виртуальной машины с полученными параметрами, и передает запрос в nova-compute
    на соответствующем узле.
    \item nova-compute получает образ диска от glance
    \item nova-compute запрашивает сетевые ресурсы у nova-network
    \item nova-compute запускает виртуальную машину в гипервизоре, передавая ему все
    полученные параметры, включая файл образа диска, IP-адрес и SSH-ключ.
    \item Пользователь получает IP-адрес виртуальная машина. 
    Для работы с машиной могут быть использованы протоколы SSH и VNC.
    \item Когда машина больше не нужна пользователю, он может отправить запрос в nova-api, 
    который будет передан nova-compute. В результате этого сетевые ресурсы будут освобождены,
    виртуальная машина остановлена, а виртуальный диск -- удален.
\end{itemize}

Недавним нововведением в платформе является сервис виртуальных сетей Quantum, который 
пришел на смену nova-network. Этот сервис предоставляет REST-интерфейс для управления
примитивами канального и сетевого уровня модели OSI. Реализация виртуальных сетей
поверх какой-либо реальной технологии носит название плагина. 
В стандартной поставке имеются плагины, реализующие виртуальные сети при помощи
следующих технологий:
\begin{itemize}
    \item виртуальные сетевые мосты Linux
    \item оборудование Cisco
    \item программный коммутатор Open vSwitch
\end{itemize}

Типичная реализация плагина состоит из нескольких частей:
\begin{itemize}
    \item Менеджер сетей, реализующий настройку оборудования и хранение данных.
    \item Плагин API, предоставляющий специфическую информацию о ресурсах. 
    Эта информация используется в дальнейшем клиентами для подключения к сети.
    \item Клиентский код, который производит локальные настройки для подключения к сети.
    При этом используется информация от API-плагина для получения нужных параметров.
\end{itemize}








